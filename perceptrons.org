#+TITLE: Lectures
#+LATEX_HEADER: \newcommand{\vv}[1]{\boldsymbol{#1}}
#+LATEX_HEADER: \usepackage{commath}
#+LATEX_HEADER: \usepackage{amsthm,amsmath,amssymb}


* Rosenblatt's perceptron 
** Introduction
/Recall/: for an input $x\in\mathop{\mathbb{R}}^n$, the parametrized function describing the mapping computed by a perceptron is:
\[
    F(\omega,x)=F((\omega_1,\omega_2,\ldots,\omega_n,\theta),(x_1,x_2,\ldots,x_n,-1))=sgn(\sum_{i=1}^n \omega_i x_i - \theta)
\]
Or in the vector form:
\[
    F(\vv{\omega}, \vv{x}) = sgn(\vv{\omega^Tx})
\]
This perceptron can only classify patterns which are /linearly seprable/.

*Theorem 1.* /Suppose $C_1\bigcup C_2=C$ are linearly separable classes over the training set $z\in Z^T$ with the assumption $z_t=\{\vv{x_t}, y_t\in C\}$, and perceptron's response $r_t$ with mistake $e_t=y_t-r_t\neq 0$ can be corrected by applying the learning rule to its current state $\vv{\omega}\in\vv{\Omega}^T$/:
\[
    \vv{\omega_t}=\vv{\omega_{t-1}} + e_t\vv{x_t}
\]
/Then perceptron's error correction algorithm converges in $k$ number of steps with following assumptions: training input is bounded by Euclidean norm $\norm{\vv{x_t}}\leq R$ and $e_t\vv{\omega_*^T x_t}\geq\gamma$ for $t=1..T$, where $\gamma > 0$. Initial state $\vv{\omega_0}=0$. Note, that $\gamma$ uses to be sure that some example is classified correctly./

*Proof:* Multiplying both sides of learning rule equation by some optimal $\vv{\omega_*^T}$ we will have:
\[
    \vv{\omega_*^T\omega_k} = \vv{\omega_*^T\omega_{k-1}} + e_k\vv{\omega_*^Tx_k}\geq\vv{\omega_*^T\omega_{k-1}} + \gamma 
\]
Now we can expand equation above for $k$ steps and keep in mind $\vv{\omega_0}=0$ we will get:
\[
    \vv{\omega_*^T\omega_k}\geq\vv{\omega_*^T}(\vv{\omega_{k-2}}+e_{k-1}\vv{x_{k-1}}) + \gamma\geq\vv{\omega_*^T}(\vv{\omega_{k-3}}+e_{k-2}\vv{x_{k-2}}) + 2\gamma\geq\ldots
\]
\[
    \ldots\geq\vv{\omega_*^T}(\vv{\omega_0}+e_1x_1)+(k-1)\gamma\geq k\gamma
\]
Let's do one important step which results will be substituted to the final inequality. Suppose we have following Euclidean norm $\norm{\vv{\omega_k}}$ and, as it's known, for squared $L2$ norm the equality holds:
\[
    \norm{\vv{\omega_k}}^2=\norm{\vv{\omega_{k-1}}+e_k\vv{x_k}}^2 = \norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2 + 2e_k\vv{\omega_{k-1}^Tx_k}
\]
Since $e_t\neq 0$ there is a misclassification for two possible cases: if for $x_t\in C_1$ the error $e_t>0$, then $sgn(\cdot)<0$; if for $x_t\in C_2$ the error $e_t<0$, then $sgn(\cdot)>0$. Thus, for any misclassification the signs of error and argument of function $sgn$ are always opposite. Therefore, with $e_k\vv{\omega_{k-1}^Tx_k}<0$ there is no doubt that:
\[
    \norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2 +2e_k\vv{\omega_{k-1}^Tx_k}\leq\norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2
\]
Continuing for $k$ steps we will have:
\[
     \norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2 +2e_k\vv{\omega_{k-1}^Tx_k}\leq e_k^2\sum_{j=1}^{k}\norm{\vv{x_j}}^2
\]
Since $\norm{\vv{x_t}}\leq R$:
\[
    \norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2 +2e_k\vv{\omega_{k-1}^Tx_k}\leq e_k^2kR^2
\]
From /Cauchy-Schwarz inequality/:
\[
    \abs{\vv{\omega_*^T\omega_k}}\leq\norm{\vv{\omega_*^T}}  \cdot     \norm{\vv{\omega_k}}  
\]
\[
    k\gamma\leq\norm{\vv{\omega_*^T}}\cdot\norm{\vv{\omega_k}}
\]
\[
    \frac{k\gamma}{\norm{\omega_*^T}}\leq\norm{\vv{\omega_k}}
\]
Substitying results obtained earlier:
\[
    \frac{k^2\gamma^2}{\norm{\omega_*^T}^2}\leq\norm{\vv{\omega_k}}^2=
    \norm{\vv{\omega_{k-1}}}^2 + e_k^2\norm{\vv{x_k}}^2 +2e_k\vv{\omega_{k-1}^Tx_k}\leq e_k^2kR^2
\]
Finally:
\[
 \frac{k^2\gamma^2}{\norm{\vv{\omega_*^T}}^2}\leq e_k^2kR^2
\]
\[
 \frac{k\gamma^2}{\norm{\vv{\omega_*^T}}^2}\leq e_k^2R^2
\]
For some finite $\vv{\omega_*^T}$:
\[
 k\leq \frac{e_k^2R^2}{\gamma^2}\norm{\vv{\omega_*^T}}^2 \blacksquare
\]
Note, that the error $e_k$ can be only $\pm 2$, thus:
\[
 k\leq \frac{4R^2}{\gamma^2}\norm{\vv{\omega_*^T}}^2
\]
