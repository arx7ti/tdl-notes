% Created 2021-01-05 Tue 21:41
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Ilia Kamyshev}
\date{\today}
\title{Lectures}
\hypersetup{
 pdfauthor={Ilia Kamyshev},
 pdftitle={Lectures},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{The Learning Problem}
\label{sec:org4fe85e1}
\subsection{Introduction}
\label{sec:org55a0734}
It is assumed that training data is randomly generated. It is also assumed that there is a probability distribution \(P\) defined on \(Z\). \(P\) is fixed and unknown for given problem. A sequence of labelled examples of the form \((x,y)\) is presented to the neural network during training. For some positive integer \(m\) there is training sample:
\[
    z=((x_1,y_1),\ldots,(x_m,y_m))=(z_1,\ldots,z_m)\in Z^m
\]
Random training sample of length \(m\) is an element of \(Z^m\) distributed according to the product probability distribution \(P^m\).
Let's denote the set of all functions the network can approximate as \(H\). Then, an \emph{error} of \(h\in H\) will be:
\[
    error_P(h)=P\{(x,y)\in Z : h(x)\neq y\}
\]
The sample error (\emph{observed error}) is defined as:
\[
    error_z(h)=\frac{1}{m}|\{i: 1\leq i \leq m\ \text{and}\ h(x_i)\neq y_i\}|
\]
Given \(h\) after the training is \emph{hypothesis} (the error of this function should have minimum value), so:
\[
    opt_P(H) = \inf_{g\in H}error_P(g)
\]
We may say that \(h\) is \(\epsilon-\text{good}\) if for \(\epsilon\in (0,1)\):
\[
    error_P(h) < opt_P(H) + \epsilon 
\]
\subsection{Formal definition of learning}
\label{sec:org1c37238}
Let's denote \(\delta\) as a \emph{confidence parameter} to ensure that the learning algrotihm will be \(\epsilon-\text{good}\) with probability at least \(1-\delta\). 
Suppose that \(H\) maps from a set \(X\) to \(\{0,1\}\). A learning algorithm \(L\) for \(H\) is a \emph{function}:
\[
    L : \bigcup_{m=1}^{\infty}Z^m\rightarrow H
\]

So if \(z\) is a training sample drawn randomly from distribution \(P^m\), then the hypothesis \(L(z)\) is such that:
\[
    error_P(L(z))<opt_P(H) + \epsilon
\]
In other words:
\[
    P^m\{error_P(L(z))<opt_P(H)+\epsilon\}\geq 1 - \delta
\]
\end{document}
